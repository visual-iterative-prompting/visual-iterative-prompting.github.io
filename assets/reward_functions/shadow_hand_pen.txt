import torch
from typing import Tuple, Dict

@torch.jit.script
def compute_reward_improved(right_hand_pos: torch.Tensor, left_hand_pos: torch.Tensor, pen_right_handle_pos: torch.Tensor,
                             pen_left_handle_pos: torch.Tensor, object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
    device = right_hand_pos.device

    # Reward for moving both hands closer to their respective pen handle positions
    right_hand_dist = torch.norm(right_hand_pos - pen_right_handle_pos, dim=-1)
    left_hand_dist = torch.norm(left_hand_pos - pen_left_handle_pos, dim=-1)
    hands_to_handles_dist = right_hand_dist + left_hand_dist

    # Temperature for scaling hands-to-handles reward
    hands_to_handles_temp: float = 5.0
    hands_to_handles_reward = 0.5 * torch.sigmoid(-hands_to_handles_temp * (hands_to_handles_dist - 0.1))

    # Reward for pulling the pen cap off the pen
    pen_cap_dist = torch.norm(object_pos - goal_pos, dim=-1)

    # Temperature for scaling pen cap reward
    pen_cap_temp: float = 50.0
    pen_cap_reward = torch.sigmoid(-pen_cap_temp * (pen_cap_dist - 0.1))

    # Reward for pulling pen handles in opposite directions
    pen_handles_dist = torch.norm(pen_right_handle_pos - pen_left_handle_pos, dim=-1)

    # Temperature for scaling pen handles pull reward
    pen_handles_pull_temp: float = 5.0
    pen_handles_pull_reward = torch.sigmoid(pen_handles_pull_temp * (pen_handles_dist - 0.1))

    # Calculate total reward as a weighted sum of individual rewards
    total_reward = hands_to_handles_reward + pen_cap_reward + pen_handles_pull_reward

    return total_reward, {"hands_to_handles_reward": hands_to_handles_reward, "pen_cap_reward": pen_cap_reward, "pen_handles_pull_reward": pen_handles_pull_reward}
