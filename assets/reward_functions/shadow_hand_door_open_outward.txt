import torch
from torch import Tensor
from typing import Tuple, Dict

@torch.jit.script
def compute_reward(left_hand_pos: Tensor, door_left_handle_pos: Tensor, door_left_handle_rot: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:
    # Define the temperature parameters for reward components
    handle_temperature: float = 0.5
    door_position_temperature: float = 5.0
    door_orientation_temperature: float = 0.5

    # Calculate the distance between the left hand and the door left handle position
    handle_dist = torch.norm(left_hand_pos - door_left_handle_pos, dim=-1)

    # Calculate the door position reward based on door"s rotation (assuming the door is perfectly closed when the quaternion is (1, 0, 0, 0))
    door_closed_quaternion = torch.tensor([1.0, 0.0, 0.0, 0.0], device=door_left_handle_rot.device)
    door_position_reward = torch.exp(-door_position_temperature * (1 - torch.sum(door_left_handle_rot * door_closed_quaternion, dim=-1)))

    # Calculate the door orientation reward based on the angle difference between the door"s current orientation and the goal orientation
    dot_product = torch.sum(door_left_handle_rot * door_closed_quaternion, dim=-1)
    angle_diff = 2 * torch.acos(torch.clamp(dot_product, -1, 1))
    door_orientation_reward = torch.exp(-door_orientation_temperature * angle_diff)

    # Calculate the handle reward
    handle_reward = torch.exp(-handle_temperature * handle_dist)

    # Calculate the total reward
    total_reward = door_position_reward + handle_reward + door_orientation_reward

    # Store individual reward components in a dictionary
    reward_dict = {"door_position_reward": door_position_reward, "handle_reward": handle_reward, "door_orientation_reward": door_orientation_reward}

    return total_reward, reward_dict
