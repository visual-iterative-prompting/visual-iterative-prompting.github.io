import torch
from torch import Tensor
from typing import Dict, Tuple

@torch.jit.script
def compute_reward(root_positions: Tensor, root_linvels: Tensor, root_angvels: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:
    quadcopter_x = root_positions[..., 0]
    quadcopter_y = root_positions[..., 1]
    quadcopter_z = root_positions[..., 2]

    target_x = 0.0
    target_y = 0.0
    target_z = 1.0

    squared_distance = (quadcopter_x - target_x)**2 + (quadcopter_y - target_y)**2 + (quadcopter_z - target_z)**2

    linvel_magnitude = torch.sqrt(torch.sum(root_linvels**2, dim=-1))
    angvel_magnitude = torch.sqrt(torch.sum(root_angvels**2, dim=-1))

    # Change temperature parameters and re-scale
    sq_distance_temperature = 1.0
    linvel_temperature = 0.075  # Adjusted from 0.05 to 0.075
    angvel_temperature = 0.005  # Adjusted from 0.01 to 0.005
    angvel_penalty_temperature = 0.05  # Adjusted from 0.1 to 0.05

    # Compute the reward components and apply transformations
    sq_distance_reward = torch.exp(-sq_distance_temperature * squared_distance)
    linvel_reward = torch.exp(-linvel_temperature * (linvel_magnitude - 0.5)) - 1.0  # Modified to encourage variance in linvel_reward
    angvel_reward = torch.exp(-angvel_temperature * angvel_magnitude)
    angvel_penalty = -torch.exp(angvel_penalty_temperature * (angvel_magnitude - 0.5))  # Modified to encourage variance in angvel_penalty

    # Check success condition
    success_condition = (squared_distance < 0.01).type(torch.float32)
    success_reward = 15 * success_condition  # Increased the success reward from 10 to 15

    # Calculate the total reward
    total_reward = success_reward + sq_distance_reward + linvel_reward + angvel_reward + angvel_penalty

    # Return the total reward and the components
    return total_reward, {"success_reward": success_reward, "sq_distance_reward": sq_distance_reward, "linvel_reward": linvel_reward, "angvel_reward": angvel_reward, "angvel_penalty": angvel_penalty}
