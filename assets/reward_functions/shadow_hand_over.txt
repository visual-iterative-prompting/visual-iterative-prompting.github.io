import torch
from typing import Dict, Tuple

@torch.jit.script
def compute_reward_v5(object_pos: torch.Tensor, object_rot: torch.Tensor, object_linvel: torch.Tensor,
                     goal_pos: torch.Tensor, goal_rot: torch.Tensor,
                     fingertip_pos: torch.Tensor, fingertip_another_pos: torch.Tensor) \
        -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:

    # Constants
    distance_threshold: float = 0.03
    rotation_threshold: float = 0.1
    catch_reward_weight: float = 30.0
    toss_reward_weight: float = 10.0
    toss_temperature: float = 1.5
    catch_temperature: float = 0.5
    penalty_weight: float = 10.0

    # Compute distance and rotation differences between object and goal
    object_goal_distance = torch.norm(object_pos - goal_pos, dim=-1)
    object_goal_rotation_diff = torch.norm(object_rot - goal_rot, dim=-1)

    # Reward for object being close to the goal position and rotation
    toss_reward = torch.exp(-toss_reward_weight * (object_goal_distance / toss_temperature))

    # Calculate the catch_reward based on the difference between the object"s linear velocity and the goal position
    catch_reward = torch.norm(goal_pos - object_linvel, dim=-1)
    catch_reward = torch.exp(-catch_reward_weight * (catch_reward / catch_temperature))

    # Penalty for direct rolling or touching the target instead of tossing and catching
    penalty = torch.zeros_like(object_goal_distance)
    for i in range(fingertip_pos.shape[1]):
        dist_to_fingertip = torch.norm(object_pos - fingertip_pos[:, i, :], dim=-1)
        dist_to_fingertip_another = torch.norm(object_pos - fingertip_another_pos[:, i, :], dim=-1)
        penalty = torch.where((dist_to_fingertip < distance_threshold) |
                              (dist_to_fingertip_another < distance_threshold),
                              penalty + 1.0, penalty)

    penalty_ratio = torch.sigmoid(penalty * penalty_weight) * 0.5
    penalty = catch_reward * penalty_ratio  # Adapt the penalty to the catch_reward"s dynamic range

    # Total reward
    total_reward = toss_reward + catch_reward - penalty
    reward_terms = {"toss_reward": toss_reward, "catch_reward": catch_reward, "penalty": penalty}

    return total_reward, reward_termsimport torch
from typing import Dict, Tuple

@torch.jit.script
def compute_reward_v5(object_pos: torch.Tensor, object_rot: torch.Tensor, object_linvel: torch.Tensor,
                     goal_pos: torch.Tensor, goal_rot: torch.Tensor,
                     fingertip_pos: torch.Tensor, fingertip_another_pos: torch.Tensor) \
        -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:

    # Constants
    distance_threshold: float = 0.03
    rotation_threshold: float = 0.1
    catch_reward_weight: float = 30.0
    toss_reward_weight: float = 10.0
    toss_temperature: float = 1.5
    catch_temperature: float = 0.5
    penalty_weight: float = 10.0

    # Compute distance and rotation differences between object and goal
    object_goal_distance = torch.norm(object_pos - goal_pos, dim=-1)
    object_goal_rotation_diff = torch.norm(object_rot - goal_rot, dim=-1)

    # Reward for object being close to the goal position and rotation
    toss_reward = torch.exp(-toss_reward_weight * (object_goal_distance / toss_temperature))

    # Calculate the catch_reward based on the difference between the object"s linear velocity and the goal position
    catch_reward = torch.norm(goal_pos - object_linvel, dim=-1)
    catch_reward = torch.exp(-catch_reward_weight * (catch_reward / catch_temperature))

    # Penalty for direct rolling or touching the target instead of tossing and catching
    penalty = torch.zeros_like(object_goal_distance)
    for i in range(fingertip_pos.shape[1]):
        dist_to_fingertip = torch.norm(object_pos - fingertip_pos[:, i, :], dim=-1)
        dist_to_fingertip_another = torch.norm(object_pos - fingertip_another_pos[:, i, :], dim=-1)
        penalty = torch.where((dist_to_fingertip < distance_threshold) |
                              (dist_to_fingertip_another < distance_threshold),
                              penalty + 1.0, penalty)

    penalty_ratio = torch.sigmoid(penalty * penalty_weight) * 0.5
    penalty = catch_reward * penalty_ratio  # Adapt the penalty to the catch_reward"s dynamic range

    # Total reward
    total_reward = toss_reward + catch_reward - penalty
    reward_terms = {"toss_reward": toss_reward, "catch_reward": catch_reward, "penalty": penalty}

    return total_reward, reward_terms

