import torch
from typing import Dict, Tuple

@torch.jit.script
def compute_reward(object_rot: torch.Tensor, goal_rot: torch.Tensor,
                   object_another_rot: torch.Tensor, goal_another_rot: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:

    # Compute the differences in quaternions between the objects" orientations and the goal orientations
    object_rot_diff = torch.abs(object_rot - goal_rot)
    object_another_rot_diff = torch.abs(object_another_rot - goal_another_rot)

    # Calculate the distance between actual and goal orientations using L2 norm
    object_rot_distance = torch.norm(object_rot_diff, dim=1)
    object_another_rot_distance = torch.norm(object_another_rot_diff, dim=1)

    # Adjust temperature parameters for transforming reward components
    temp_object_rot = torch.tensor(8.0, dtype=torch.float32, device=object_rot.device)
    temp_object_another_rot = torch.tensor(5.0, dtype=torch.float32, device=object_another_rot.device)

    # Transform reward components using exponential
    object_rotation_reward = torch.exp(-temp_object_rot * object_rot_distance)
    object_another_rotation_reward = torch.exp(-temp_object_another_rot * object_another_rot_distance)

    # Update the orientation success thresholds and scale the success bonuses
    orientation_success_threshold = torch.tensor(0.02, dtype=torch.float32, device=object_rot.device)
    success_bonus_scale = torch.tensor(5.0, dtype=torch.float32, device=object_rot.device)
    object_orientation_success_bonus = torch.where(object_rot_distance < orientation_success_threshold, success_bonus_scale * (1.0 - object_rot_distance), torch.tensor(0.0, dtype=torch.float32, device=object_rot.device))
    object_another_orientation_success_bonus = torch.where(object_another_rot_distance < orientation_success_threshold, success_bonus_scale * (1.0 - object_another_rot_distance), torch.tensor(0.0, dtype=torch.float32, device=object_another_rot.device))

    # Calculate the total reward as the sum of individual rewards
    reward = object_rotation_reward + object_another_rotation_reward + object_orientation_success_bonus + object_another_orientation_success_bonus

    # Return the total reward and a dictionary containing the individual reward components
    reward_components = {
        "object_rotation_reward": object_rotation_reward,
        "object_another_rotation_reward": object_another_rotation_reward,
        "object_orientation_success_bonus": object_orientation_success_bonus,
        "object_another_orientation_success_bonus": object_another_orientation_success_bonus,
    }
    return reward, reward_components
