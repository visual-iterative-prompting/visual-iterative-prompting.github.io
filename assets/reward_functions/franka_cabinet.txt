import torch
from typing import Tuple, Dict

@torch.jit.script
def compute_reward(franka_grasp_pos: torch.Tensor, drawer_grasp_pos: torch.Tensor,
                   cabinet_dof_pos: torch.Tensor, cabinet_dof_vel: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:

    # Calculate the distance between the hand grasp position and the drawer grasp position
    grasp_dist = torch.norm(franka_grasp_pos - drawer_grasp_pos, dim=-1)

    # Reward for being close to the drawer handle
    grasp_reward_temp = 10.0
    grasp_reward = torch.exp(-grasp_reward_temp * grasp_dist)

    # Reward for opening the cabinet door
    door_open_reward_temp = 50.0
    door_open_reward_raw = torch.clamp(cabinet_dof_pos[:, 3], 0.0, 1.0)
    door_open_reward = torch.sigmoid(door_open_reward_temp * (door_open_reward_raw - 0.5))

    # Encourage the agent to pull the drawer by increasing distance between drawer and cabinet origin
    drawer_dist_reward_temp = 20.0
    drawer_dist_reward = torch.sigmoid(drawer_dist_reward_temp * (cabinet_dof_pos[:, 3] - 0.5))

    # Penalty for high velocity while opening the cabinet door
    door_open_vel_penalty_temp = 2.0
    door_open_vel_penalty = torch.exp(-door_open_vel_penalty_temp * torch.abs(cabinet_dof_vel[:, 3]))

    # Combine rewards and penalties
    total_reward = grasp_reward + door_open_reward + drawer_dist_reward - door_open_vel_penalty

    # Store the individual components as a dictionary
    reward_components = {"grasp_reward": grasp_reward,
                         "door_open_reward": door_open_reward,
                         "drawer_dist_reward": drawer_dist_reward,
                         "door_open_vel_penalty": door_open_vel_penalty}

    return total_reward, reward_components
