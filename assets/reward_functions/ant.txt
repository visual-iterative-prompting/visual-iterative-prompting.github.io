import torch
from typing import Dict, Tuple

@torch.jit.script
def compute_reward(root_states: torch.Tensor, targets: torch.Tensor, potentials: torch.Tensor, dt: float) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
    # Compute distance between ant"s current position and its forward target
    torso_position = root_states[:, 0:3]
    to_target = targets - torso_position
    to_target[:, 2] = 0.0

    # Compute progress towards the forward target
    prev_potentials_new = potentials.clone()
    progress = -torch.norm(to_target, p=2, dim=-1) / dt

    # Calculate the step reward for forward progress (negative distance to target)
    forward_reward = progress - prev_potentials_new
    forward_reward_temperature = 0.1  # Added temperature for forward_reward scaling
    forward_normalized_reward = torch.exp(forward_reward / forward_reward_temperature)

    # Compute a reward component for the current velocity
    velocity = root_states[:, 7:10]
    forward_velocity = velocity[:, 0]
    forward_velocity_temperature = 1.0  # Adjusted temperature for velocity_reward scaling
    forward_velocity_normalized_reward = torch.exp(forward_velocity / forward_velocity_temperature)

    # Add a penalty term for the agent"s body height deviation from the target height
    target_height = 0.4
    height_penalty = torch.abs(torso_position[:, 2] - target_height)
    height_penalty_temperature = 0.1  # Adjusted temperature for height_penalty scaling
    height_normalized_penalty = torch.exp(-height_penalty / height_penalty_temperature)

    # Compute total reward and individual reward components
    reward = forward_normalized_reward * forward_velocity_normalized_reward * height_normalized_penalty
    reward_components = {"forward_reward": forward_normalized_reward,
                         "velocity_reward": forward_velocity_normalized_reward,
                         "height_penalty": height_normalized_penalty}

    return reward, reward_components
